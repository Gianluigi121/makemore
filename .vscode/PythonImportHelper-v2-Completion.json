[
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "words",
        "kind": 5,
        "importPath": "NN",
        "description": "NN",
        "peekOfCode": "words = open('names.txt', 'r').read().split()\n# Create the reference dictionary\nchs = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i, s in enumerate(chs)}\nstoi['.'] = 0\nitos = {i:s for s, i in stoi.items()}\n# Create a dataset\nxs = []\nys = []\nfor word in words:",
        "detail": "NN",
        "documentation": {}
    },
    {
        "label": "chs",
        "kind": 5,
        "importPath": "NN",
        "description": "NN",
        "peekOfCode": "chs = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i, s in enumerate(chs)}\nstoi['.'] = 0\nitos = {i:s for s, i in stoi.items()}\n# Create a dataset\nxs = []\nys = []\nfor word in words:\n    chs = '.' + word + '.'\n    for ch1, ch2 in zip(chs, chs[1:]):",
        "detail": "NN",
        "documentation": {}
    },
    {
        "label": "stoi",
        "kind": 5,
        "importPath": "NN",
        "description": "NN",
        "peekOfCode": "stoi = {s:i+1 for i, s in enumerate(chs)}\nstoi['.'] = 0\nitos = {i:s for s, i in stoi.items()}\n# Create a dataset\nxs = []\nys = []\nfor word in words:\n    chs = '.' + word + '.'\n    for ch1, ch2 in zip(chs, chs[1:]):\n        idx1 = stoi[ch1]",
        "detail": "NN",
        "documentation": {}
    },
    {
        "label": "stoi['.']",
        "kind": 5,
        "importPath": "NN",
        "description": "NN",
        "peekOfCode": "stoi['.'] = 0\nitos = {i:s for s, i in stoi.items()}\n# Create a dataset\nxs = []\nys = []\nfor word in words:\n    chs = '.' + word + '.'\n    for ch1, ch2 in zip(chs, chs[1:]):\n        idx1 = stoi[ch1]\n        idx2 = stoi[ch2]",
        "detail": "NN",
        "documentation": {}
    },
    {
        "label": "itos",
        "kind": 5,
        "importPath": "NN",
        "description": "NN",
        "peekOfCode": "itos = {i:s for s, i in stoi.items()}\n# Create a dataset\nxs = []\nys = []\nfor word in words:\n    chs = '.' + word + '.'\n    for ch1, ch2 in zip(chs, chs[1:]):\n        idx1 = stoi[ch1]\n        idx2 = stoi[ch2]\n        xs.append(idx1)",
        "detail": "NN",
        "documentation": {}
    },
    {
        "label": "xs",
        "kind": 5,
        "importPath": "NN",
        "description": "NN",
        "peekOfCode": "xs = []\nys = []\nfor word in words:\n    chs = '.' + word + '.'\n    for ch1, ch2 in zip(chs, chs[1:]):\n        idx1 = stoi[ch1]\n        idx2 = stoi[ch2]\n        xs.append(idx1)\n        ys.append(idx2)\nx = torch.tensor(xs)",
        "detail": "NN",
        "documentation": {}
    },
    {
        "label": "ys",
        "kind": 5,
        "importPath": "NN",
        "description": "NN",
        "peekOfCode": "ys = []\nfor word in words:\n    chs = '.' + word + '.'\n    for ch1, ch2 in zip(chs, chs[1:]):\n        idx1 = stoi[ch1]\n        idx2 = stoi[ch2]\n        xs.append(idx1)\n        ys.append(idx2)\nx = torch.tensor(xs)\ny = torch.tensor(ys)",
        "detail": "NN",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "NN",
        "description": "NN",
        "peekOfCode": "x = torch.tensor(xs)\ny = torch.tensor(ys)\n# Initialize the weights\ng = torch.Generator().manual_seed(0)     \nw = torch.randn((27, 27), generator=g, requires_grad=True)\n# Training loop\nepoch = 200\nnum_samples = len(xs)\nstep_size = 50\nfor i in range(epoch):",
        "detail": "NN",
        "documentation": {}
    },
    {
        "label": "y",
        "kind": 5,
        "importPath": "NN",
        "description": "NN",
        "peekOfCode": "y = torch.tensor(ys)\n# Initialize the weights\ng = torch.Generator().manual_seed(0)     \nw = torch.randn((27, 27), generator=g, requires_grad=True)\n# Training loop\nepoch = 200\nnum_samples = len(xs)\nstep_size = 50\nfor i in range(epoch):\n    # Forward pass",
        "detail": "NN",
        "documentation": {}
    },
    {
        "label": "g",
        "kind": 5,
        "importPath": "NN",
        "description": "NN",
        "peekOfCode": "g = torch.Generator().manual_seed(0)     \nw = torch.randn((27, 27), generator=g, requires_grad=True)\n# Training loop\nepoch = 200\nnum_samples = len(xs)\nstep_size = 50\nfor i in range(epoch):\n    # Forward pass\n    x_enc = F.one_hot(x, num_classes=27).float()\n    logits = x_enc @ w",
        "detail": "NN",
        "documentation": {}
    },
    {
        "label": "w",
        "kind": 5,
        "importPath": "NN",
        "description": "NN",
        "peekOfCode": "w = torch.randn((27, 27), generator=g, requires_grad=True)\n# Training loop\nepoch = 200\nnum_samples = len(xs)\nstep_size = 50\nfor i in range(epoch):\n    # Forward pass\n    x_enc = F.one_hot(x, num_classes=27).float()\n    logits = x_enc @ w\n    counts = torch.exp(logits)",
        "detail": "NN",
        "documentation": {}
    },
    {
        "label": "epoch",
        "kind": 5,
        "importPath": "NN",
        "description": "NN",
        "peekOfCode": "epoch = 200\nnum_samples = len(xs)\nstep_size = 50\nfor i in range(epoch):\n    # Forward pass\n    x_enc = F.one_hot(x, num_classes=27).float()\n    logits = x_enc @ w\n    counts = torch.exp(logits)\n    P = counts / counts.sum(dim=1, keepdims=True)\n    loss = -P[torch.arange(num_samples), ys].log().mean()",
        "detail": "NN",
        "documentation": {}
    },
    {
        "label": "num_samples",
        "kind": 5,
        "importPath": "NN",
        "description": "NN",
        "peekOfCode": "num_samples = len(xs)\nstep_size = 50\nfor i in range(epoch):\n    # Forward pass\n    x_enc = F.one_hot(x, num_classes=27).float()\n    logits = x_enc @ w\n    counts = torch.exp(logits)\n    P = counts / counts.sum(dim=1, keepdims=True)\n    loss = -P[torch.arange(num_samples), ys].log().mean()\n    # Backward pass",
        "detail": "NN",
        "documentation": {}
    },
    {
        "label": "step_size",
        "kind": 5,
        "importPath": "NN",
        "description": "NN",
        "peekOfCode": "step_size = 50\nfor i in range(epoch):\n    # Forward pass\n    x_enc = F.one_hot(x, num_classes=27).float()\n    logits = x_enc @ w\n    counts = torch.exp(logits)\n    P = counts / counts.sum(dim=1, keepdims=True)\n    loss = -P[torch.arange(num_samples), ys].log().mean()\n    # Backward pass\n    w.grad = None",
        "detail": "NN",
        "documentation": {}
    },
    {
        "label": "word_num",
        "kind": 5,
        "importPath": "NN",
        "description": "NN",
        "peekOfCode": "word_num = 5\nfor i in range(word_num):\n    idx = 0\n    out = []\n    while True:\n        x_enc = F.one_hot(torch.tensor(idx), num_classes = 27).float()\n        logits = x_enc @ w\n        counts = logits.exp()\n        p = counts / counts.sum()\n        idx = torch.multinomial(p, num_samples=1, generator=g).item()",
        "detail": "NN",
        "documentation": {}
    },
    {
        "label": "words",
        "kind": 5,
        "importPath": "bigram",
        "description": "bigram",
        "peekOfCode": "words = open('names.txt', 'r').read().split()\n# Step 1: Build a bigram dictionary\n# key: Bigram tuple, value: Number of appearence that they occur together\nb = {}\nfor word in words:\n    word = '.' + word + '.'\n    for ch1, ch2 in zip(word, word[1:]):\n        bigram = (ch1, ch2)\n        b[bigram] = b.get(bigram, 0)+1\n# Step 2: Create reference dictionaries",
        "detail": "bigram",
        "documentation": {}
    },
    {
        "label": "b",
        "kind": 5,
        "importPath": "bigram",
        "description": "bigram",
        "peekOfCode": "b = {}\nfor word in words:\n    word = '.' + word + '.'\n    for ch1, ch2 in zip(word, word[1:]):\n        bigram = (ch1, ch2)\n        b[bigram] = b.get(bigram, 0)+1\n# Step 2: Create reference dictionaries\n# Create a dictionary map from chars to integers\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i, s in enumerate(chars)}",
        "detail": "bigram",
        "documentation": {}
    },
    {
        "label": "chars",
        "kind": 5,
        "importPath": "bigram",
        "description": "bigram",
        "peekOfCode": "chars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i, s in enumerate(chars)}\nstoi['.'] = 0\n# Create a dictionary map from integers to chars\nitos = {i:s for s, i in stoi.items()}\n# Step 3: Create a torch tensor where each entry represent the number of occurrence\nN = torch.zeros((27, 27))\nfor bigram, count in b.items():\n    ch1, ch2 = bigram\n    id1 = stoi[ch1]",
        "detail": "bigram",
        "documentation": {}
    },
    {
        "label": "stoi",
        "kind": 5,
        "importPath": "bigram",
        "description": "bigram",
        "peekOfCode": "stoi = {s:i+1 for i, s in enumerate(chars)}\nstoi['.'] = 0\n# Create a dictionary map from integers to chars\nitos = {i:s for s, i in stoi.items()}\n# Step 3: Create a torch tensor where each entry represent the number of occurrence\nN = torch.zeros((27, 27))\nfor bigram, count in b.items():\n    ch1, ch2 = bigram\n    id1 = stoi[ch1]\n    id2 = stoi[ch2]",
        "detail": "bigram",
        "documentation": {}
    },
    {
        "label": "stoi['.']",
        "kind": 5,
        "importPath": "bigram",
        "description": "bigram",
        "peekOfCode": "stoi['.'] = 0\n# Create a dictionary map from integers to chars\nitos = {i:s for s, i in stoi.items()}\n# Step 3: Create a torch tensor where each entry represent the number of occurrence\nN = torch.zeros((27, 27))\nfor bigram, count in b.items():\n    ch1, ch2 = bigram\n    id1 = stoi[ch1]\n    id2 = stoi[ch2]\n    N[id1][id2] += count",
        "detail": "bigram",
        "documentation": {}
    },
    {
        "label": "itos",
        "kind": 5,
        "importPath": "bigram",
        "description": "bigram",
        "peekOfCode": "itos = {i:s for s, i in stoi.items()}\n# Step 3: Create a torch tensor where each entry represent the number of occurrence\nN = torch.zeros((27, 27))\nfor bigram, count in b.items():\n    ch1, ch2 = bigram\n    id1 = stoi[ch1]\n    id2 = stoi[ch2]\n    N[id1][id2] += count\n# Step 4: Show the figure of the matrix\n# plt.figure(figsize=(16,16))",
        "detail": "bigram",
        "documentation": {}
    },
    {
        "label": "N",
        "kind": 5,
        "importPath": "bigram",
        "description": "bigram",
        "peekOfCode": "N = torch.zeros((27, 27))\nfor bigram, count in b.items():\n    ch1, ch2 = bigram\n    id1 = stoi[ch1]\n    id2 = stoi[ch2]\n    N[id1][id2] += count\n# Step 4: Show the figure of the matrix\n# plt.figure(figsize=(16,16))\n# plt.imshow(N, cmap='Blues')\n# for i in range(27):",
        "detail": "bigram",
        "documentation": {}
    },
    {
        "label": "P",
        "kind": 5,
        "importPath": "bigram",
        "description": "bigram",
        "peekOfCode": "P = N / torch.sum(N, dim=1, keepdims=True)\nprint(P[0].sum())\n# Step 6: Inference: Use the bigram matrix, start with . and end when we see another .\ng = torch.Generator().manual_seed(0)\nword_num = 5\nfor i in range(word_num):\n    idx = 0\n    out = []\n    while True:\n        p = P[idx] # prob of the next char given the current one",
        "detail": "bigram",
        "documentation": {}
    },
    {
        "label": "g",
        "kind": 5,
        "importPath": "bigram",
        "description": "bigram",
        "peekOfCode": "g = torch.Generator().manual_seed(0)\nword_num = 5\nfor i in range(word_num):\n    idx = 0\n    out = []\n    while True:\n        p = P[idx] # prob of the next char given the current one\n        idx = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n        if idx == 0:\n            break",
        "detail": "bigram",
        "documentation": {}
    },
    {
        "label": "word_num",
        "kind": 5,
        "importPath": "bigram",
        "description": "bigram",
        "peekOfCode": "word_num = 5\nfor i in range(word_num):\n    idx = 0\n    out = []\n    while True:\n        p = P[idx] # prob of the next char given the current one\n        idx = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n        if idx == 0:\n            break\n        out.append(itos[idx])",
        "detail": "bigram",
        "documentation": {}
    },
    {
        "label": "Linear",
        "kind": 6,
        "importPath": "part5(Wavenet)",
        "description": "part5(Wavenet)",
        "peekOfCode": "class Linear:\n    def __init__(self, fan_in, fan_out, bias=True):\n        self.weight = torch.randn(fan_in, fan_out) / (fan_in ** 0.5)\n        self.bias = torch.zeros(fan_out) if bias else None\n    def __call__(self, x):\n        self.out = x @ self.weight\n        if self.bias is not None:\n            self.out += self.bias\n        return self.out\n    def parameters(self):",
        "detail": "part5(Wavenet)",
        "documentation": {}
    },
    {
        "label": "BatchNorm1D",
        "kind": 6,
        "importPath": "part5(Wavenet)",
        "description": "part5(Wavenet)",
        "peekOfCode": "class BatchNorm1D:\n    def __init__(self, dim, eps=1e-5, momentum=0.01):\n        self.eps = eps\n        self.momentum = momentum\n        self.training = True\n        # Init params\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n        # Init running updates\n        self.running_mean = torch.zeros(dim)",
        "detail": "part5(Wavenet)",
        "documentation": {}
    },
    {
        "label": "Tanh",
        "kind": 6,
        "importPath": "part5(Wavenet)",
        "description": "part5(Wavenet)",
        "peekOfCode": "class Tanh:\n    def __call__(self, x):\n        self.out = torch.tanh(x)\n        return self.out\n    def parameters(self):\n        return []\nclass Embedding:\n    def __init__(self, vocab_size, emb_size):\n        self.weight = torch.randn(vocab_size, emb_size)\n    def __call__(self, x):",
        "detail": "part5(Wavenet)",
        "documentation": {}
    },
    {
        "label": "Embedding",
        "kind": 6,
        "importPath": "part5(Wavenet)",
        "description": "part5(Wavenet)",
        "peekOfCode": "class Embedding:\n    def __init__(self, vocab_size, emb_size):\n        self.weight = torch.randn(vocab_size, emb_size)\n    def __call__(self, x):\n        # X: (batch_size, block_size) self.out: (batch_size, block_size, emb_size)\n        self.out = self.weight[x] \n        return self.out\n    def parameters(self):\n        return [self.weight]\nclass Flatten:",
        "detail": "part5(Wavenet)",
        "documentation": {}
    },
    {
        "label": "Flatten",
        "kind": 6,
        "importPath": "part5(Wavenet)",
        "description": "part5(Wavenet)",
        "peekOfCode": "class Flatten:\n    def __init__(self, group_size):\n        self.group_size = group_size\n    def __call__(self, x):\n        batch_size, block_size, emb_size = x.shape\n        self.out = x.view(batch_size, block_size // self.group_size, self.group_size * emb_size)\n        if self.out.shape[1] == 1:\n            self.out = self.out.squeeze(dim=1)\n        return self.out\n    def parameters(self):",
        "detail": "part5(Wavenet)",
        "documentation": {}
    },
    {
        "label": "Sequential",
        "kind": 6,
        "importPath": "part5(Wavenet)",
        "description": "part5(Wavenet)",
        "peekOfCode": "class Sequential:\n    # Layers: A list of layers\n    def __init__(self, layers):\n        self.layers = layers\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        self.out = x\n        return self.out\n    def parameters(self):",
        "detail": "part5(Wavenet)",
        "documentation": {}
    },
    {
        "label": "build_dataset",
        "kind": 2,
        "importPath": "part5(Wavenet)",
        "description": "part5(Wavenet)",
        "peekOfCode": "def build_dataset(words):\n    xs = []\n    ys = []\n    for word in words:\n        word += '.'\n        context = [0]*block_size\n        for ch in word:\n            xs.append(context)\n            idx = stoi[ch]\n            ys.append(idx)",
        "detail": "part5(Wavenet)",
        "documentation": {}
    },
    {
        "label": "split_loss",
        "kind": 2,
        "importPath": "part5(Wavenet)",
        "description": "part5(Wavenet)",
        "peekOfCode": "def split_loss(split):\n    X, Y = {\n        'train': (Xtr, Ytr), \n        'val': (Xval, Yval),\n        'test': (Xtest, Ytest)\n    }[split]\n    logits = model(X)\n    loss = F.cross_entropy(logits, Y)\n    print(f\"{split} loss: {loss.item()}\")\nsplit_loss('train')",
        "detail": "part5(Wavenet)",
        "documentation": {}
    },
    {
        "label": "words",
        "kind": 5,
        "importPath": "part5(Wavenet)",
        "description": "part5(Wavenet)",
        "peekOfCode": "words = open('names.txt', 'r').read().split()\nprint(\"read_file\")\n# Build a reference map\nchs = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i, s in enumerate(chs)}\nstoi['.'] = 0\nitos = {i:s for s, i in stoi.items()}\nblock_size = 8\n# Build the dataset\ndef build_dataset(words):",
        "detail": "part5(Wavenet)",
        "documentation": {}
    },
    {
        "label": "chs",
        "kind": 5,
        "importPath": "part5(Wavenet)",
        "description": "part5(Wavenet)",
        "peekOfCode": "chs = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i, s in enumerate(chs)}\nstoi['.'] = 0\nitos = {i:s for s, i in stoi.items()}\nblock_size = 8\n# Build the dataset\ndef build_dataset(words):\n    xs = []\n    ys = []\n    for word in words:",
        "detail": "part5(Wavenet)",
        "documentation": {}
    },
    {
        "label": "stoi",
        "kind": 5,
        "importPath": "part5(Wavenet)",
        "description": "part5(Wavenet)",
        "peekOfCode": "stoi = {s:i+1 for i, s in enumerate(chs)}\nstoi['.'] = 0\nitos = {i:s for s, i in stoi.items()}\nblock_size = 8\n# Build the dataset\ndef build_dataset(words):\n    xs = []\n    ys = []\n    for word in words:\n        word += '.'",
        "detail": "part5(Wavenet)",
        "documentation": {}
    },
    {
        "label": "stoi['.']",
        "kind": 5,
        "importPath": "part5(Wavenet)",
        "description": "part5(Wavenet)",
        "peekOfCode": "stoi['.'] = 0\nitos = {i:s for s, i in stoi.items()}\nblock_size = 8\n# Build the dataset\ndef build_dataset(words):\n    xs = []\n    ys = []\n    for word in words:\n        word += '.'\n        context = [0]*block_size",
        "detail": "part5(Wavenet)",
        "documentation": {}
    },
    {
        "label": "itos",
        "kind": 5,
        "importPath": "part5(Wavenet)",
        "description": "part5(Wavenet)",
        "peekOfCode": "itos = {i:s for s, i in stoi.items()}\nblock_size = 8\n# Build the dataset\ndef build_dataset(words):\n    xs = []\n    ys = []\n    for word in words:\n        word += '.'\n        context = [0]*block_size\n        for ch in word:",
        "detail": "part5(Wavenet)",
        "documentation": {}
    },
    {
        "label": "block_size",
        "kind": 5,
        "importPath": "part5(Wavenet)",
        "description": "part5(Wavenet)",
        "peekOfCode": "block_size = 8\n# Build the dataset\ndef build_dataset(words):\n    xs = []\n    ys = []\n    for word in words:\n        word += '.'\n        context = [0]*block_size\n        for ch in word:\n            xs.append(context)",
        "detail": "part5(Wavenet)",
        "documentation": {}
    },
    {
        "label": "num1",
        "kind": 5,
        "importPath": "part5(Wavenet)",
        "description": "part5(Wavenet)",
        "peekOfCode": "num1 = int(len(words)*0.8)\nnum2 = int(len(words)*0.9)\nXtr, Ytr = build_dataset(words[:num1])\nXval, Yval = build_dataset(words[num1:num2])\nXtest, Ytest = build_dataset(words[num2:])\n# Define Model layers\nclass Linear:\n    def __init__(self, fan_in, fan_out, bias=True):\n        self.weight = torch.randn(fan_in, fan_out) / (fan_in ** 0.5)\n        self.bias = torch.zeros(fan_out) if bias else None",
        "detail": "part5(Wavenet)",
        "documentation": {}
    },
    {
        "label": "num2",
        "kind": 5,
        "importPath": "part5(Wavenet)",
        "description": "part5(Wavenet)",
        "peekOfCode": "num2 = int(len(words)*0.9)\nXtr, Ytr = build_dataset(words[:num1])\nXval, Yval = build_dataset(words[num1:num2])\nXtest, Ytest = build_dataset(words[num2:])\n# Define Model layers\nclass Linear:\n    def __init__(self, fan_in, fan_out, bias=True):\n        self.weight = torch.randn(fan_in, fan_out) / (fan_in ** 0.5)\n        self.bias = torch.zeros(fan_out) if bias else None\n    def __call__(self, x):",
        "detail": "part5(Wavenet)",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "part5(Wavenet)",
        "description": "part5(Wavenet)",
        "peekOfCode": "batch_size = 32\nblock_size = 8\nemb_size = 10\nn_hidden = 100\nvocab_size = len(itos)\ngroup_size = 2\nmodel = Sequential([\n    Embedding(vocab_size, emb_size), # (32, 8, 10)\n    Flatten(2),  # (32, 4, 20)\n    Linear(group_size * emb_size, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),  # (32, 4, 100)",
        "detail": "part5(Wavenet)",
        "documentation": {}
    },
    {
        "label": "block_size",
        "kind": 5,
        "importPath": "part5(Wavenet)",
        "description": "part5(Wavenet)",
        "peekOfCode": "block_size = 8\nemb_size = 10\nn_hidden = 100\nvocab_size = len(itos)\ngroup_size = 2\nmodel = Sequential([\n    Embedding(vocab_size, emb_size), # (32, 8, 10)\n    Flatten(2),  # (32, 4, 20)\n    Linear(group_size * emb_size, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),  # (32, 4, 100)\n    Flatten(2),  # (32, 2, 200)",
        "detail": "part5(Wavenet)",
        "documentation": {}
    },
    {
        "label": "emb_size",
        "kind": 5,
        "importPath": "part5(Wavenet)",
        "description": "part5(Wavenet)",
        "peekOfCode": "emb_size = 10\nn_hidden = 100\nvocab_size = len(itos)\ngroup_size = 2\nmodel = Sequential([\n    Embedding(vocab_size, emb_size), # (32, 8, 10)\n    Flatten(2),  # (32, 4, 20)\n    Linear(group_size * emb_size, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),  # (32, 4, 100)\n    Flatten(2),  # (32, 2, 200)\n    Linear(group_size * n_hidden, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),  # (32, 2, 100)",
        "detail": "part5(Wavenet)",
        "documentation": {}
    },
    {
        "label": "n_hidden",
        "kind": 5,
        "importPath": "part5(Wavenet)",
        "description": "part5(Wavenet)",
        "peekOfCode": "n_hidden = 100\nvocab_size = len(itos)\ngroup_size = 2\nmodel = Sequential([\n    Embedding(vocab_size, emb_size), # (32, 8, 10)\n    Flatten(2),  # (32, 4, 20)\n    Linear(group_size * emb_size, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),  # (32, 4, 100)\n    Flatten(2),  # (32, 2, 200)\n    Linear(group_size * n_hidden, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),  # (32, 2, 100)\n    Flatten(2),  # (32, 200)",
        "detail": "part5(Wavenet)",
        "documentation": {}
    },
    {
        "label": "vocab_size",
        "kind": 5,
        "importPath": "part5(Wavenet)",
        "description": "part5(Wavenet)",
        "peekOfCode": "vocab_size = len(itos)\ngroup_size = 2\nmodel = Sequential([\n    Embedding(vocab_size, emb_size), # (32, 8, 10)\n    Flatten(2),  # (32, 4, 20)\n    Linear(group_size * emb_size, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),  # (32, 4, 100)\n    Flatten(2),  # (32, 2, 200)\n    Linear(group_size * n_hidden, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),  # (32, 2, 100)\n    Flatten(2),  # (32, 200)\n    Linear(group_size * n_hidden, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),  # (32, 100)",
        "detail": "part5(Wavenet)",
        "documentation": {}
    },
    {
        "label": "group_size",
        "kind": 5,
        "importPath": "part5(Wavenet)",
        "description": "part5(Wavenet)",
        "peekOfCode": "group_size = 2\nmodel = Sequential([\n    Embedding(vocab_size, emb_size), # (32, 8, 10)\n    Flatten(2),  # (32, 4, 20)\n    Linear(group_size * emb_size, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),  # (32, 4, 100)\n    Flatten(2),  # (32, 2, 200)\n    Linear(group_size * n_hidden, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),  # (32, 2, 100)\n    Flatten(2),  # (32, 200)\n    Linear(group_size * n_hidden, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),  # (32, 100)\n    Linear(n_hidden, vocab_size), BatchNorm1D(vocab_size)   # (32, 27)",
        "detail": "part5(Wavenet)",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "part5(Wavenet)",
        "description": "part5(Wavenet)",
        "peekOfCode": "model = Sequential([\n    Embedding(vocab_size, emb_size), # (32, 8, 10)\n    Flatten(2),  # (32, 4, 20)\n    Linear(group_size * emb_size, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),  # (32, 4, 100)\n    Flatten(2),  # (32, 2, 200)\n    Linear(group_size * n_hidden, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),  # (32, 2, 100)\n    Flatten(2),  # (32, 200)\n    Linear(group_size * n_hidden, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),  # (32, 100)\n    Linear(n_hidden, vocab_size), BatchNorm1D(vocab_size)   # (32, 27)\n])",
        "detail": "part5(Wavenet)",
        "documentation": {}
    },
    {
        "label": "parameters",
        "kind": 5,
        "importPath": "part5(Wavenet)",
        "description": "part5(Wavenet)",
        "peekOfCode": "parameters = model.parameters()\n# Set the gradient for all params to be true\nfor p in parameters:\n    p.requires_grad = True\n# Training\nepoch_num = 200000\nlossi = []\nfor i in range(epoch_num):\n    # Select a mini batch\n    idx = torch.randint(0, Xtr.shape[0], (batch_size, ))",
        "detail": "part5(Wavenet)",
        "documentation": {}
    },
    {
        "label": "epoch_num",
        "kind": 5,
        "importPath": "part5(Wavenet)",
        "description": "part5(Wavenet)",
        "peekOfCode": "epoch_num = 200000\nlossi = []\nfor i in range(epoch_num):\n    # Select a mini batch\n    idx = torch.randint(0, Xtr.shape[0], (batch_size, ))\n    Xb, Yb = Xtr[idx], Ytr[idx]\n    # Forward pass\n    logits = model(Xb)\n    loss = F.cross_entropy(logits, Yb)\n    lossi.append(loss.log10().item())",
        "detail": "part5(Wavenet)",
        "documentation": {}
    },
    {
        "label": "lossi",
        "kind": 5,
        "importPath": "part5(Wavenet)",
        "description": "part5(Wavenet)",
        "peekOfCode": "lossi = []\nfor i in range(epoch_num):\n    # Select a mini batch\n    idx = torch.randint(0, Xtr.shape[0], (batch_size, ))\n    Xb, Yb = Xtr[idx], Ytr[idx]\n    # Forward pass\n    logits = model(Xb)\n    loss = F.cross_entropy(logits, Yb)\n    lossi.append(loss.log10().item())\n    # Backward",
        "detail": "part5(Wavenet)",
        "documentation": {}
    },
    {
        "label": "word_num",
        "kind": 5,
        "importPath": "part5(Wavenet)",
        "description": "part5(Wavenet)",
        "peekOfCode": "word_num = 20\nfor i in range(word_num):\n    out = []\n    context = [0]*block_size\n    while True:\n        x = torch.tensor(context).view(1, -1)  # (1, block_size)\n        logits = model(x) # (1, vocab_size)\n        prob = F.softmax(logits, dim=1).view(vocab_size)\n        idx = torch.multinomial(prob, num_samples=1).item()\n        out.append(itos[idx])",
        "detail": "part5(Wavenet)",
        "documentation": {}
    }
]